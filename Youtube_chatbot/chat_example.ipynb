{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d4ffdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_google_genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586fb3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_huggingface\n",
      "  Using cached langchain_huggingface-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain_huggingface)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langchain_huggingface) (1.1.0)\n",
      "Collecting tokenizers<1.0.0,>=0.19.1 (from langchain_huggingface)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (6.0.3)\n",
      "Requirement already satisfied: requests in d:\\project-to-learn\\.langchain\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.32.5)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.15.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.4.53)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (2.12.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.0.0)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.12.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.25.0)\n",
      "Requirement already satisfied: anyio in d:\\project-to-learn\\.langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (4.12.0)\n",
      "Requirement already satisfied: certifi in d:\\project-to-learn\\.langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\project-to-learn\\.langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\project-to-learn\\.langchain\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.1.0->langchain_huggingface) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\project-to-learn\\.langchain\\lib\\site-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2.5.0)\n",
      "Requirement already satisfied: colorama in d:\\project-to-learn\\.langchain\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (0.4.6)\n",
      "Using cached langchain_huggingface-1.1.0-py3-none-any.whl (29 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, fsspec, filelock, huggingface-hub, tokenizers, langchain_huggingface\n",
      "\n",
      "   ---------------------------------------- 0/6 [tqdm]\n",
      "   ---------------------------------------- 0/6 [tqdm]\n",
      "   ---------------------------------------- 0/6 [tqdm]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------ --------------------------------- 1/6 [fsspec]\n",
      "   ------------- -------------------------- 2/6 [filelock]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------- ------------------- 3/6 [huggingface-hub]\n",
      "   -------------------------- ------------- 4/6 [tokenizers]\n",
      "   -------------------------- ------------- 4/6 [tokenizers]\n",
      "   --------------------------------- ------ 5/6 [langchain_huggingface]\n",
      "   ---------------------------------------- 6/6 [langchain_huggingface]\n",
      "\n",
      "Successfully installed filelock-3.20.0 fsspec-2025.10.0 huggingface-hub-0.36.0 langchain_huggingface-1.1.0 tokenizers-0.22.1 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aa07c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install FAISS-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31a8d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42054aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_list=[{'text': 'Imagine you happen across a short movie script that',\n",
    "  'start': 1.14,\n",
    "  'duration': 2.836},\n",
    " {'text': 'describes a scene between a person and their AI assistant.',\n",
    "  'start': 3.976,\n",
    "  'duration': 3.164},\n",
    " {'text': \"The script has what the person asks the AI, but the AI's response has been torn off.\",\n",
    "  'start': 7.48,\n",
    "  'duration': 5.58},\n",
    " {'text': 'Suppose you also have this powerful magical machine that can take',\n",
    "  'start': 13.06,\n",
    "  'duration': 3.92},\n",
    " {'text': 'any text and provide a sensible prediction of what word comes next.',\n",
    "  'start': 16.98,\n",
    "  'duration': 3.98},\n",
    " {'text': 'You could then finish the script by feeding in what you have to the machine,',\n",
    "  'start': 21.5,\n",
    "  'duration': 4.006},\n",
    " {'text': \"seeing what it would predict to start the AI's answer,\",\n",
    "  'start': 25.506,\n",
    "  'duration': 2.862},\n",
    " {'text': 'and then repeating this over and over with a growing script completing the dialogue.',\n",
    "  'start': 28.368,\n",
    "  'duration': 4.372},\n",
    " {'text': \"When you interact with a chatbot, this is exactly what's happening.\",\n",
    "  'start': 33.38,\n",
    "  'duration': 3.1},\n",
    " {'text': 'A large language model is a sophisticated mathematical function',\n",
    "  'start': 37.02,\n",
    "  'duration': 3.681},\n",
    " {'text': 'that predicts what word comes next for any piece of text.',\n",
    "  'start': 40.701,\n",
    "  'duration': 3.279},\n",
    " {'text': 'Instead of predicting one word with certainty, though,',\n",
    "  'start': 44.38,\n",
    "  'duration': 3.022},\n",
    " {'text': 'what it does is assign a probability to all possible next words.',\n",
    "  'start': 47.402,\n",
    "  'duration': 3.518},\n",
    " {'text': 'To build a chatbot, you lay out some text that describes an interaction between a user',\n",
    "  'start': 51.62,\n",
    "  'duration': 5.18},\n",
    " {'text': 'and a hypothetical AI assistant, add on whatever the user types in as the first part of',\n",
    "  'start': 56.8,\n",
    "  'duration': 5.24},\n",
    " {'text': 'the interaction, and then have the model repeatedly predict the next word that such a',\n",
    "  'start': 62.04,\n",
    "  'duration': 5.12},\n",
    " {'text': \"hypothetical AI assistant would say in response, and that's what's presented to the user.\",\n",
    "  'start': 67.16,\n",
    "  'duration': 5.3},\n",
    " {'text': 'In doing this, the output tends to look a lot more natural if',\n",
    "  'start': 73.08,\n",
    "  'duration': 3.134},\n",
    " {'text': 'you allow it to select less likely words along the way at random.',\n",
    "  'start': 76.214,\n",
    "  'duration': 3.286},\n",
    " {'text': 'So what this means is even though the model itself is deterministic,',\n",
    "  'start': 80.14,\n",
    "  'duration': 3.48},\n",
    " {'text': \"a given prompt typically gives a different answer each time it's run.\",\n",
    "  'start': 83.62,\n",
    "  'duration': 3.48},\n",
    " {'text': 'Models learn how to make these predictions by processing an enormous amount of text,',\n",
    "  'start': 88.04,\n",
    "  'duration': 4.292},\n",
    " {'text': 'typically pulled from the internet.',\n",
    "  'start': 92.332,\n",
    "  'duration': 1.768},\n",
    " {'text': 'For a standard human to read the amount of text that was used to train GPT-3,',\n",
    "  'start': 94.1,\n",
    "  'duration': 5.371},\n",
    " {'text': 'for example, if they read non-stop 24-7, it would take over 2600 years.',\n",
    "  'start': 99.471,\n",
    "  'duration': 4.889},\n",
    " {'text': 'Larger models since then train on much, much more.',\n",
    "  'start': 104.72,\n",
    "  'duration': 2.62},\n",
    " {'text': 'You can think of training a little bit like tuning the dials on a big machine.',\n",
    "  'start': 108.2,\n",
    "  'duration': 3.58},\n",
    " {'text': 'The way that a language model behaves is entirely determined by these',\n",
    "  'start': 112.28,\n",
    "  'duration': 4.021},\n",
    " {'text': 'many different continuous values, usually called parameters or weights.',\n",
    "  'start': 116.301,\n",
    "  'duration': 4.079},\n",
    " {'text': 'Changing those parameters will change the probabilities',\n",
    "  'start': 121.02,\n",
    "  'duration': 3.079},\n",
    " {'text': 'that the model gives for the next word on a given input.',\n",
    "  'start': 124.099,\n",
    "  'duration': 3.081},\n",
    " {'text': 'What puts the large in large language model is how',\n",
    "  'start': 127.86,\n",
    "  'duration': 2.867},\n",
    " {'text': 'they can have hundreds of billions of these parameters.',\n",
    "  'start': 130.727,\n",
    "  'duration': 3.093},\n",
    " {'text': 'No human ever deliberately sets those parameters.',\n",
    "  'start': 135.2,\n",
    "  'duration': 2.84},\n",
    " {'text': 'Instead, they begin at random, meaning the model just outputs gibberish,',\n",
    "  'start': 138.44,\n",
    "  'duration': 4.203},\n",
    " {'text': \"but they're repeatedly refined based on many example pieces of text.\",\n",
    "  'start': 142.643,\n",
    "  'duration': 3.917},\n",
    " {'text': 'One of these training examples could be just a handful of words,',\n",
    "  'start': 147.14,\n",
    "  'duration': 3.516},\n",
    " {'text': 'or it could be thousands, but in either case, the way this works is to',\n",
    "  'start': 150.656,\n",
    "  'duration': 3.84},\n",
    " {'text': 'pass in all but the last word from that example into the model and',\n",
    "  'start': 154.496,\n",
    "  'duration': 3.624},\n",
    " {'text': 'compare the prediction that it makes with the true last word from the example.',\n",
    "  'start': 158.12,\n",
    "  'duration': 4.22},\n",
    " {'text': 'An algorithm called backpropagation is used to tweak all of the parameters',\n",
    "  'start': 163.26,\n",
    "  'duration': 4.133},\n",
    " {'text': 'in such a way that it makes the model a little more likely to choose',\n",
    "  'start': 167.393,\n",
    "  'duration': 3.803},\n",
    " {'text': 'the true last word and a little less likely to choose all the others.',\n",
    "  'start': 171.196,\n",
    "  'duration': 3.804},\n",
    " {'text': 'When you do this for many, many trillions of examples,',\n",
    "  'start': 175.74,\n",
    "  'duration': 3.01},\n",
    " {'text': 'not only does the model start to give more accurate predictions on the training data,',\n",
    "  'start': 178.75,\n",
    "  'duration': 4.708},\n",
    " {'text': \"but it also starts to make more reasonable predictions on text that it's never\",\n",
    "  'start': 183.458,\n",
    "  'duration': 4.325},\n",
    " {'text': 'seen before.', 'start': 187.783, 'duration': 0.657},\n",
    " {'text': 'Given the huge number of parameters and the enormous amount of training data,',\n",
    "  'start': 189.42,\n",
    "  'duration': 4.499},\n",
    " {'text': 'the scale of computation involved in training a large language model is mind-boggling.',\n",
    "  'start': 193.919,\n",
    "  'duration': 4.961},\n",
    " {'text': 'To illustrate, imagine that you could perform one',\n",
    "  'start': 199.6,\n",
    "  'duration': 2.685},\n",
    " {'text': 'billion additions and multiplications every single second.',\n",
    "  'start': 202.285,\n",
    "  'duration': 3.115},\n",
    " {'text': 'How long do you think it would take for you to do all of the',\n",
    "  'start': 206.06,\n",
    "  'duration': 3.266},\n",
    " {'text': 'operations involved in training the largest language models?',\n",
    "  'start': 209.326,\n",
    "  'duration': 3.214},\n",
    " {'text': 'Do you think it would take a year?',\n",
    "  'start': 213.46,\n",
    "  'duration': 1.579},\n",
    " {'text': 'Maybe something like 10,000 years?',\n",
    "  'start': 216.039,\n",
    "  'duration': 1.921},\n",
    " {'text': 'The answer is actually much more than that.',\n",
    "  'start': 219.02,\n",
    "  'duration': 1.78},\n",
    " {'text': \"It's well over 100 million years.\",\n",
    "  'start': 221.12,\n",
    "  'duration': 2.78},\n",
    " {'text': 'This is only part of the story, though.',\n",
    "  'start': 225.52,\n",
    "  'duration': 1.84},\n",
    " {'text': 'This whole process is called pre-training.',\n",
    "  'start': 227.54,\n",
    "  'duration': 1.68},\n",
    " {'text': 'The goal of auto-completing a random passage of text from the',\n",
    "  'start': 229.5,\n",
    "  'duration': 3.146},\n",
    " {'text': 'internet is very different from the goal of being a good AI assistant.',\n",
    "  'start': 232.646,\n",
    "  'duration': 3.554},\n",
    " {'text': 'To address this, chatbots undergo another type of training,',\n",
    "  'start': 236.88,\n",
    "  'duration': 3.2},\n",
    " {'text': 'just as important, called reinforcement learning with human feedback.',\n",
    "  'start': 240.08,\n",
    "  'duration': 3.68},\n",
    " {'text': 'Workers flag unhelpful or problematic predictions,',\n",
    "  'start': 244.48,\n",
    "  'duration': 3.018},\n",
    " {'text': \"and their corrections further change the model's parameters,\",\n",
    "  'start': 247.498,\n",
    "  'duration': 3.611},\n",
    " {'text': 'making them more likely to give predictions that users prefer.',\n",
    "  'start': 251.109,\n",
    "  'duration': 3.671},\n",
    " {'text': 'Looking back at the pre-training, though, this staggering amount of',\n",
    "  'start': 254.78,\n",
    "  'duration': 4.08},\n",
    " {'text': 'computation is only made possible by using special computer chips that',\n",
    "  'start': 258.86,\n",
    "  'duration': 4.26},\n",
    " {'text': 'are optimized for running many operations in parallel, known as GPUs.',\n",
    "  'start': 263.12,\n",
    "  'duration': 4.14},\n",
    " {'text': 'However, not all language models can be easily parallelized.',\n",
    "  'start': 268.12,\n",
    "  'duration': 3.5},\n",
    " {'text': 'Prior to 2017, most language models would process text one word at a time,',\n",
    "  'start': 272.08,\n",
    "  'duration': 4.737},\n",
    " {'text': 'but then a team of researchers at Google introduced a new model known as the transformer.',\n",
    "  'start': 276.817,\n",
    "  'duration': 5.623},\n",
    " {'text': \"Transformers don't read text from the start to the finish,\",\n",
    "  'start': 283.3,\n",
    "  'duration': 3.445},\n",
    " {'text': 'they soak it all in at once, in parallel.',\n",
    "  'start': 286.745,\n",
    "  'duration': 2.395},\n",
    " {'text': 'The very first step inside a transformer, and most other language models for that matter,',\n",
    "  'start': 289.9,\n",
    "  'duration': 4.7},\n",
    " {'text': 'is to associate each word with a long list of numbers.',\n",
    "  'start': 294.6,\n",
    "  'duration': 2.82},\n",
    " {'text': 'The reason for this is that the training process only works with continuous values,',\n",
    "  'start': 297.86,\n",
    "  'duration': 4.536},\n",
    " {'text': 'so you have to somehow encode language using numbers,',\n",
    "  'start': 302.396,\n",
    "  'duration': 2.916},\n",
    " {'text': 'and each of these lists of numbers may somehow encode the meaning of the',\n",
    "  'start': 305.312,\n",
    "  'duration': 3.942},\n",
    " {'text': 'corresponding word.', 'start': 309.254, 'duration': 1.026},\n",
    " {'text': 'What makes transformers unique is their reliance',\n",
    "  'start': 310.28,\n",
    "  'duration': 3.08},\n",
    " {'text': 'on a special operation known as attention.',\n",
    "  'start': 313.36,\n",
    "  'duration': 2.64},\n",
    " {'text': 'This operation gives all of these lists of numbers a chance to talk to one another',\n",
    "  'start': 316.98,\n",
    "  'duration': 4.704},\n",
    " {'text': 'and refine the meanings they encode based on the context around, all done in parallel.',\n",
    "  'start': 321.684,\n",
    "  'duration': 4.876},\n",
    " {'text': 'For example, the numbers encoding the word bank might be changed based on the',\n",
    "  'start': 327.4,\n",
    "  'duration': 4.307},\n",
    " {'text': 'context surrounding it to somehow encode the more specific notion of a riverbank.',\n",
    "  'start': 331.707,\n",
    "  'duration': 4.473},\n",
    " {'text': 'Transformers typically also include a second type of operation known',\n",
    "  'start': 337.28,\n",
    "  'duration': 3.749},\n",
    " {'text': 'as a feed-forward neural network, and this gives the model extra',\n",
    "  'start': 341.029,\n",
    "  'duration': 3.532},\n",
    " {'text': 'capacity to store more patterns about language learned during training.',\n",
    "  'start': 344.561,\n",
    "  'duration': 3.859},\n",
    " {'text': 'All of this data repeatedly flows through many different iterations of',\n",
    "  'start': 349.28,\n",
    "  'duration': 4.121},\n",
    " {'text': 'these two fundamental operations, and as it does so,',\n",
    "  'start': 353.401,\n",
    "  'duration': 3.077},\n",
    " {'text': 'the hope is that each list of numbers is enriched to encode whatever',\n",
    "  'start': 356.478,\n",
    "  'duration': 4.006},\n",
    " {'text': 'information might be needed to make an accurate prediction of what word',\n",
    "  'start': 360.484,\n",
    "  'duration': 4.18},\n",
    " {'text': 'follows in the passage.', 'start': 364.664, 'duration': 1.336},\n",
    " {'text': 'At the end, one final function is performed on the last vector in this sequence,',\n",
    "  'start': 367.0,\n",
    "  'duration': 4.534},\n",
    " {'text': 'which now has had a chance to be influenced by all the other context from the input text,',\n",
    "  'start': 371.534,\n",
    "  'duration': 5.039},\n",
    " {'text': 'as well as everything the model learned during training,',\n",
    "  'start': 376.573,\n",
    "  'duration': 3.191},\n",
    " {'text': 'to produce a prediction of the next word.',\n",
    "  'start': 379.764,\n",
    "  'duration': 2.296},\n",
    " {'text': \"Again, the model's prediction looks like a probability for every possible next word.\",\n",
    "  'start': 382.48,\n",
    "  'duration': 4.88},\n",
    " {'text': 'Although researchers design the framework for how each of these steps work,',\n",
    "  'start': 388.56,\n",
    "  'duration': 4.234},\n",
    " {'text': \"it's important to understand that the specific behavior is an emergent phenomenon\",\n",
    "  'start': 392.794,\n",
    "  'duration': 4.568},\n",
    " {'text': 'based on how those hundreds of billions of parameters are tuned during training.',\n",
    "  'start': 397.362,\n",
    "  'duration': 4.458},\n",
    " {'text': 'This makes it incredibly challenging to determine',\n",
    "  'start': 402.48,\n",
    "  'duration': 2.59},\n",
    " {'text': 'why the model makes the exact predictions that it does.',\n",
    "  'start': 405.07,\n",
    "  'duration': 2.85},\n",
    " {'text': 'What you can see is that when you use large language model predictions to autocomplete',\n",
    "  'start': 408.44,\n",
    "  'duration': 5.338},\n",
    " {'text': 'a prompt, the words that it generates are uncannily fluent, fascinating, and even useful.',\n",
    "  'start': 413.778,\n",
    "  'duration': 5.462},\n",
    " {'text': \"If you're a new viewer and you're curious about more details on how\",\n",
    "  'start': 425.719,\n",
    "  'duration': 3.107},\n",
    " {'text': 'transformers and attention work, boy do I have some material for you.',\n",
    "  'start': 428.826,\n",
    "  'duration': 3.153},\n",
    " {'text': 'One option is to jump into a series I made about deep learning,',\n",
    "  'start': 432.399,\n",
    "  'duration': 3.681},\n",
    " {'text': 'where we visualize and motivate the details of attention and all the other steps',\n",
    "  'start': 436.08,\n",
    "  'duration': 4.66},\n",
    " {'text': 'in a transformer.', 'start': 440.74, 'duration': 0.979},\n",
    " {'text': 'Also, on my second channel I just posted a talk I gave a couple',\n",
    "  'start': 442.099,\n",
    "  'duration': 3.43},\n",
    " {'text': 'months ago about this topic for the company TNG in Munich.',\n",
    "  'start': 445.529,\n",
    "  'duration': 3.11},\n",
    " {'text': 'Sometimes I actually prefer the content I make as a casual talk rather than a produced',\n",
    "  'start': 449.079,\n",
    "  'duration': 4.022},\n",
    " {'text': 'video, but I leave it up to you which one of these feels like the better follow-on.',\n",
    "  'start': 453.101,\n",
    "  'duration': 3.838}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2cba6579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff83c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_id = \"Gfr50f6ZBvo\" # only the ID, not full URL\n",
    "# try:\n",
    "#     # If you don’t care which language, this returns the “best” one\n",
    "#     transcript_list = YouTubeTranscriptApi.get_transcript(video_id, languages=[\"en\"])\n",
    "\n",
    "#     # Flatten it to plain text\n",
    "#     transcript = \" \".join(chunk[\"text\"] for chunk in transcript_list)\n",
    "#     print(transcript)\n",
    "\n",
    "# except TranscriptsDisabled:\n",
    "#     print(\"No captions available for this video.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7c61367",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript=\" \".join(chunk[\"text\"] for chunk in transcript_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f21ba",
   "metadata": {},
   "source": [
    "Step 1b - Indexing (Text Splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c83c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "chunks = splitter.create_documents([transcript])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57bd3938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chucnks[100]\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d842269e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"person and their AI assistant. The script has what the person asks the AI, but the AI's response\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e4897de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d4c1b399",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding=HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6dda8b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(\n",
    "    chunks, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2d7b4d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '7efa62ea-9c69-48a5-9490-55924b72cd01',\n",
       " 1: '7602b89f-0586-448c-a3c2-9fe34cf0f90f',\n",
       " 2: '4857124c-03c1-43a6-b02b-a695311c4e1d',\n",
       " 3: '9edce7bc-47d9-4e56-b058-7ec603a33ce9',\n",
       " 4: '3f6a10f7-b294-475e-b507-723015629263',\n",
       " 5: '7749e2ad-eadd-4b27-9aa1-984dd2a27ac6',\n",
       " 6: '9ca5c673-ed92-47bd-a286-ce433b06e23e',\n",
       " 7: 'bd28e415-f5ff-4c78-bc2e-19660585d02c',\n",
       " 8: 'd70d457c-1c2c-44ac-897d-96d097251b8c',\n",
       " 9: '0913959c-a8ce-4903-b225-9f7051af7c11',\n",
       " 10: 'e8ebc591-ed8a-4c97-b128-79e22afe9307',\n",
       " 11: 'f3ff4dc2-1e60-46cf-baba-eb006c9abd19',\n",
       " 12: '00ba8939-cdc9-4843-b87c-564413fee989',\n",
       " 13: '7fc533c2-d3db-469b-b215-698d7ca0036c',\n",
       " 14: '459e5d41-d50f-4d31-9ba6-b08f1b941494',\n",
       " 15: 'fac4e9c3-6ac7-485d-9184-0e16b43ebf7f',\n",
       " 16: '055a7dcb-30d2-4bdf-80c5-bb55582b39e8',\n",
       " 17: '2f5e7ea5-5168-4ff5-9147-930ca4c99574',\n",
       " 18: 'e5abd677-0247-469b-8c5b-8daf78c72ef4',\n",
       " 19: 'cde50574-5101-4c0d-b9ce-607ddac1e6dc',\n",
       " 20: '22b25176-25e9-4eb7-ac50-586e416fd418',\n",
       " 21: '1b4cd6c1-1d55-4c4e-bf05-a2695cce1ffa',\n",
       " 22: 'bf5c395a-07a7-4658-a3fc-2be0fcf3db2f',\n",
       " 23: 'c9d80e14-db01-4414-9cc8-001a28456b11',\n",
       " 24: '0db2d9df-5cc4-4ced-b820-cbc8a38c887f',\n",
       " 25: 'dc17dd28-b232-43ca-b351-6fc34a05d470',\n",
       " 26: 'a58e75c3-15fd-470b-8744-6698b64119e8',\n",
       " 27: '7cace1ae-e3fc-47b2-a86a-61678ed45e6e',\n",
       " 28: 'b0ffadd6-46a3-4ba5-94f5-03146a0fde22',\n",
       " 29: '1d722111-12e2-4ffc-9ef9-483c607ba4e2',\n",
       " 30: '863468e8-72ae-456c-bd3a-6486abda3fc6',\n",
       " 31: '3d9f0872-d931-4b99-b0d8-9172fabe63bf',\n",
       " 32: '3814371b-58d9-4a3f-acc7-5593fc002f97',\n",
       " 33: 'b1a8e4a9-da2d-49ad-a532-034e690c9b7f',\n",
       " 34: '6d66ce36-18e2-42b0-9744-09486b67055d',\n",
       " 35: '494bfe3a-0614-4557-bfe9-5eb9f0cd88e1',\n",
       " 36: '4e461e92-8186-4692-ac26-24869e7d7ca8',\n",
       " 37: '7ff18e80-2157-49b7-87a7-368b06954af5',\n",
       " 38: '81ed0f91-fb15-4d73-9dc5-c616d52c4edb',\n",
       " 39: '6ada65f3-61f8-46b4-b87a-1862e4a449d4',\n",
       " 40: '7f9980d2-53c9-4b40-b299-fa597fffb579',\n",
       " 41: 'cb8b7a0d-8370-421d-b65f-acd685ac467a',\n",
       " 42: '62c7ffd3-8e62-45d8-bb36-8cdb21022c22',\n",
       " 43: '915ae538-6cd9-4539-be04-2f23cfc8ccdd',\n",
       " 44: '5dc8e087-eca5-4caf-a70e-fd051ff08ce4',\n",
       " 45: '58ecc9c6-717d-4d7f-b216-781d8e11cab9',\n",
       " 46: 'c3f46f3b-ba0b-4c75-80af-4b04025cea09',\n",
       " 47: '22c1c8fa-ad1b-4e25-8a6b-f2e9079ac911',\n",
       " 48: '3f3264b4-f335-4d01-bdef-ae103f117511',\n",
       " 49: '05c1ffb7-d49e-4989-9034-bd6901db3d58',\n",
       " 50: 'acdd302d-22b6-4f80-a786-627f54e93e11',\n",
       " 51: '5ee5e5a0-5eb8-4d8d-8b8f-ce3f4e0545c8',\n",
       " 52: 'd9a8e23c-5bed-4574-9ebe-4b261c0ce36c',\n",
       " 53: '08dc330f-6069-422e-b66e-afe341a02af4',\n",
       " 54: 'e844dbd0-9726-474d-bd1b-c3f218f35fcd',\n",
       " 55: '2d3c2787-4ac2-45ce-865f-70348075eb06',\n",
       " 56: 'f07f1345-19fd-4513-8ccb-c70768dc0687',\n",
       " 57: 'd51cd5b0-71dc-4060-87ba-9bb11079214b',\n",
       " 58: '28cdcfb4-b258-4787-8c2d-c4b25a6fee85',\n",
       " 59: 'dfeeb059-dffe-4e31-9ea9-0e46ee8f2a17',\n",
       " 60: 'd88909dc-f03d-4c72-850c-47bfc0c42d4b',\n",
       " 61: '6085200a-a573-4bcb-8e82-e015be534c8d',\n",
       " 62: '0c020eb5-25c6-41e1-b7de-2b66ad2aa74f',\n",
       " 63: 'de2f0fb0-02af-4ef3-9b04-f48fa4eb5ecf',\n",
       " 64: 'db61aacf-a85c-44d2-81fc-5fbf0a41b237',\n",
       " 65: 'c056dafe-9d62-4cd4-93ca-f38230f3bcfb',\n",
       " 66: 'ceec05b8-29a5-40f1-b39d-7f39a9e46b98',\n",
       " 67: 'e3cca287-84ea-40b5-b6ae-96639dffff23',\n",
       " 68: 'a33c03a4-a3ca-4d4f-9d5a-35b8cf15b500',\n",
       " 69: '6e921fe5-cfca-413f-8d32-b6a446a28522',\n",
       " 70: 'dabca639-576d-4d30-b754-16d06ab4bf25',\n",
       " 71: '82474ab0-0727-4d30-8b88-23fcae0f887a',\n",
       " 72: '827bcdd4-f768-47ca-9e1c-85bf21a63a86',\n",
       " 73: '9886f304-4e58-4904-943d-1220cd57c827',\n",
       " 74: 'f72b85e5-8038-45c1-a6e7-d823fbeae3c9',\n",
       " 75: 'd437e6e7-ebc5-46c0-a293-eff42a7a6192',\n",
       " 76: '9a7f2c5a-5ba3-41cb-b44a-773d95e61e8f',\n",
       " 77: 'ce8c5f9a-a639-48c4-9694-a2bfa486d383',\n",
       " 78: 'ae8a7668-86b8-4093-b63b-d0771e0fc50d',\n",
       " 79: 'b96272c1-68e2-4ccf-88b7-fe965b0eda72',\n",
       " 80: '2f300dc9-abd9-48d2-9214-b16400151d88',\n",
       " 81: '8da34151-dffd-45a7-92e4-0d3495e5e5b9',\n",
       " 82: 'c2e92c3e-cad1-4191-b9d3-63082f25c2df',\n",
       " 83: '6ec6d7a7-9a1a-4efa-bdca-4a72d8663021',\n",
       " 84: '220cf9f9-3be8-4763-918d-0ad8438d176e',\n",
       " 85: 'e091d784-9896-473d-8602-fefe2fc2765a',\n",
       " 86: 'bf3b5997-22dc-4d91-b99d-e16f7aac1917',\n",
       " 87: '600e64e3-9a87-4db6-a650-3d45b2303c4d',\n",
       " 88: '1782d1a6-a230-416d-9350-47209ba6c377',\n",
       " 89: 'dc13173a-b0ab-4898-a81d-d8e8e5c7949a',\n",
       " 90: 'aa54f817-849e-4b8b-a27a-548b322fe191',\n",
       " 91: 'e451b683-0af0-4a89-9229-bd7033c3ae1e',\n",
       " 92: '96bc2759-af9f-4dad-b31b-125b083c219c',\n",
       " 93: '93aa6e1e-853e-41cf-815c-17c3f04f65d1'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index_to_docstore_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b5e42cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='dc13173a-b0ab-4898-a81d-d8e8e5c7949a', metadata={}, page_content='of attention and all the other steps in a transformer. Also, on my second channel I just posted a')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.get_by_ids(['dc13173a-b0ab-4898-a81d-d8e8e5c7949a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0831ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64344abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002666FF8AC90>, search_kwargs={'k': 4})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".langchain (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
